import re
import json
import requests
import random
import os
import urllib.parse
import sys, traceback
import time
from bs4 import BeautifulSoup
import gspread
from googleapiclient.discovery import build
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials as UserCredentials
from google.oauth2.service_account import Credentials
from googleapiclient.http import MediaFileUpload
from openai import OpenAI
from module_1 import make_thumb, to_webp

# ================================
# ì¶œë ¥ í•œê¸€ ê¹¨ì§ ë°©ì§€
# ================================
sys.stdout.reconfigure(encoding="utf-8")

# ================================
# OpenAI API í‚¤ ë¡œë“œ
# ================================
OPENAI_API_KEY = ""
if os.path.exists("openai.json"):
    with open("openai.json", "r", encoding="utf-8") as f:
        data = json.load(f)
        OPENAI_API_KEY = data.get("api_key", "").strip()
if not OPENAI_API_KEY:
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
if not OPENAI_API_KEY:
    print("[ERROR] OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit(1)

client = OpenAI(api_key=OPENAI_API_KEY)

# ================================
# Google Sheets ì¸ì¦
# ================================
SERVICE_ACCOUNT_FILE = "sheetapi.json"
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]

creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
gc = gspread.authorize(creds)

SHEET_ID = os.getenv("SHEET_ID", "1V6ZV_b2NMlqjIobJqV5BBSr9o7_bF8WNjSIwMzQekRs")
ws = gc.open_by_key(SHEET_ID).sheet1

# ================================
# ë¸”ë¡œê·¸ ID ë¡œí…Œì´ì…˜ (O1 ì…€)
# ================================
BLOG_IDS = ["1271002762142343021", "4265887538424434999", "6159101125292617147"]

try:
    last_index = int(ws.acell("O1").value or "-1")
except:
    last_index = -1
next_index = (last_index + 1) % len(BLOG_IDS)
BLOG_ID = BLOG_IDS[next_index]
ws.update_acell("O1", str(next_index))
print(f"ğŸ‘‰ ì´ë²ˆ í¬ìŠ¤íŒ… ë¸”ë¡œê·¸ ID: {BLOG_ID}")

# ================================
# URL ê°€ì ¸ì˜¤ê¸° (Eì—´ URL, Gì—´ ìƒíƒœ)
# ================================
target_row, my_url = None, None
rows = ws.get_all_values()
for i, row in enumerate(rows[1:], start=2):
    url_cell = row[4] if len(row) > 4 else ""   # Eì—´
    status_cell = row[6] if len(row) > 6 else "" # Gì—´
    if url_cell and (not status_cell or status_cell.strip() != "ì™„"):
        my_url, target_row = url_cell, i
        break
if not my_url:
    print("ğŸ”” ì²˜ë¦¬í•  ìƒˆë¡œìš´ URLì´ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit(0)
print("ğŸ‘‰ ì´ë²ˆì— ì²˜ë¦¬í•  URL:", my_url)

parsed = urllib.parse.urlparse(my_url)
params = urllib.parse.parse_qs(parsed.query)
wlfareInfoId = params.get("wlfareInfoId", [""])[0]
print("wlfareInfoId =", wlfareInfoId)

# ================================
# ë³µì§€ ì„œë¹„ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
# ================================
def fetch_welfare_info(wlfareInfoId):
    url = f"https://www.bokjiro.go.kr/ssis-tbu/twataa/wlfareInfo/moveTWAT52011M.do?wlfareInfoId={wlfareInfoId}&wlfareInfoReldBztpCd=01"
    resp = requests.get(url)
    resp.encoding = "utf-8"
    html = resp.text
    outer_match = re.search(r'initParameter\((\{.*?\})\);', html, re.S)
    if not outer_match:
        raise ValueError("initParameter JSONì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    outer_data = json.loads(outer_match.group(1))
    inner_str = outer_data["initValue"]["dmWlfareInfo"]
    return json.loads(inner_str)

def clean_html(raw_html):
    return BeautifulSoup(raw_html, "html.parser").get_text(separator="\n", strip=True)

# ================================
# GPT ë³€í™˜
# ================================
def process_with_gpt(section_title: str, raw_text: str, keyword: str) -> str:
    system_msg = (
        "ë„ˆëŠ” í•œêµ­ì–´ ë¸”ë¡œê·¸ ê¸€ì„ ì“°ëŠ” ì¹´í”¼ë¼ì´í„°ì•¼. "
        "ì£¼ì œëŠ” ì •ë¶€ ë³µì§€ì„œë¹„ìŠ¤ì´ê³ , "
        "1) <b>êµµê²Œ ìš”ì•½</b>, "
        "2) ì´ì–´ì„œ ì¹œì ˆí•˜ê³  í’ì„±í•œ ì„¤ëª…. "
        "3~4 ë¬¸ë‹¨, ë°˜ë“œì‹œ <p data-ke-size=\"size18\"> íƒœê·¸ ì‚¬ìš©."
    )
    user_msg = f"[ì„¹ì…˜ ì œëª©] {keyword} {section_title}\n[ì›ë¬¸]\n{raw_text}"
    try:
        resp = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            temperature=0.7,
            max_tokens=900,
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        print("[WARN] GPT ì‹¤íŒ¨:", e)
        return f"<p data-ke-size='size18'>{clean_html(raw_text)}</p>"

# ================================
# ì„œë¡ Â·ë§ˆë¬´ë¦¬ ë¬¸êµ¬ (7ë¬¸ë‹¨ ëœë¤)
# ================================
synonyms = {
    "ë„ì›€": ["ë„ì›€", "ì§€ì›", "í˜œíƒ", "ë³´íƒ¬", "ì´ìµ", "ìœ ìµ", "ê±°ë“¤ìŒ", "ë’·ë°›ì¹¨", "ë³´í˜¸", "í›„ì›", "ì•ˆì •ë§"],
    "ì•ˆë‚´": ["ì•ˆë‚´", "ì†Œê°œ", "ì •ë¦¬", "ê°€ì´ë“œ", "ì„¤ëª…", "í’€ì´", "ê¸¸ì¡ì´", "í•´ì„¤", "ì•ˆë‚´ì„œ", "ì•Œë¦¼"],
    "ì¤‘ìš”í•œ": ["ì¤‘ìš”í•œ", "í•µì‹¬ì ì¸", "í•„ìˆ˜ì ì¸", "ê¼­ ì•Œì•„ì•¼ í• ", "ê°€ì¥ í° ì˜ë¯¸ê°€ ìˆëŠ”", "ìƒí™œì— í•„ìš”í•œ", "ë³¸ì§ˆì ì¸", "ì ˆëŒ€ì ì¸", "í•µì‹¬ í¬ì¸íŠ¸ê°€ ë˜ëŠ”"],
    "ì‰½ê²Œ": ["ì‰½ê²Œ", "ê°„ë‹¨íˆ", "ìˆ˜ì›”í•˜ê²Œ", "í¸ë¦¬í•˜ê²Œ", "í•œê²° ìˆ˜ì›”í•˜ê²Œ", "ë¶€ë‹´ ì—†ì´", "ë¹ ë¥´ê²Œ", "íš¨ìœ¨ì ìœ¼ë¡œ", "ì‹ ì†í•˜ê²Œ"],
    "ì •ë³´": ["ì •ë³´", "ë‚´ìš©", "ìë£Œ", "ì†Œì‹", "ë°ì´í„°", "ì†Œì‹ì§€", "ì•Œë¦¼", "ì†Œì‹ê±°ë¦¬", "í•µì‹¬ ìš”ì•½", "í•„ìš”í•œ ì§€ì‹"],
    "ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤": ["ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤", "ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤", "ì •ë¦¬í•˜ê² ìŠµë‹ˆë‹¤", "í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤", "ì°¨ê·¼ì°¨ê·¼ í’€ì–´ë³´ê² ìŠµë‹ˆë‹¤", "í•˜ë‚˜ì”© ì§šì–´ë³´ê² ìŠµë‹ˆë‹¤", "ê¼¼ê¼¼íˆ ë‹¤ë¤„ë³´ê² ìŠµë‹ˆë‹¤"],
}
def choice(word): return random.choice(synonyms.get(word, [word]))

def make_intro(keyword):
    parts = [
        [f"{keyword}ì€ ë§ì€ ë¶„ë“¤ì´ ê´€ì‹¬ì„ ê°–ëŠ” {choice('ì¤‘ìš”í•œ')} ì œë„ì…ë‹ˆë‹¤.",
         f"{keyword} ì œë„ëŠ” {choice('ì¤‘ìš”í•œ')} ë³µì§€ ì„œë¹„ìŠ¤ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤."],
        ["ì •ë¶€ëŠ” ì´ë¥¼ í†µí•´ ìƒí™œì˜ ì–´ë ¤ì›€ì„ ëœì–´ì£¼ê³ ì í•©ë‹ˆë‹¤.",
         "ì´ ì œë„ëŠ” ê²½ì œì  ë¶€ë‹´ì„ ì¤„ì´ëŠ” ë° í° ì—­í• ì„ í•©ë‹ˆë‹¤."],
        [f"ì œë„ë¥¼ ì˜ ì´í•´í•˜ë©´ í˜œíƒì„ ë”ìš± {choice('ì‰½ê²Œ')} ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
         "ì‹ ì²­ ê³¼ì •ì„ ì •í™•íˆ ì•Œë©´ ì‹œí–‰ì°©ì˜¤ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."],
        [f"ì˜¤ëŠ˜ì€ {keyword}ì˜ ê°œìš”ë¶€í„° ì‹ ì²­ ë°©ë²•ê¹Œì§€ ê¼¼ê¼¼íˆ {choice('ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤')}.",
         f"ì´ë²ˆ ê¸€ì—ì„œëŠ” {keyword}ì— ëŒ€í•´ ì „ë°˜ì ìœ¼ë¡œ {choice('ì•ˆë‚´')}í•©ë‹ˆë‹¤."],
        ["ì‹¤ì œ ìƒí™œì—ì„œ ì–´ë–»ê²Œ í™œìš©ë˜ëŠ”ì§€ ì‚¬ë¡€ë¥¼ í†µí•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
         "í˜„ì¥ì—ì„œ ìœ ìš©í•˜ê²Œ ì“°ì´ëŠ” ë°©ì•ˆë“¤ë„ í•¨ê»˜ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤."],
        ["ëê¹Œì§€ ì½ìœ¼ì‹œë©´ ì œë„ë¥¼ ì´í•´í•˜ëŠ” ë° í° ë³´íƒ¬ì´ ë˜ì‹¤ ê²ë‹ˆë‹¤.",
         "ì—¬ëŸ¬ë¶„ê»˜ ê¼­ í•„ìš”í•œ ì§€ì‹ê³¼ í˜œíƒì„ ì „í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."],
        ["ì´ ê¸€ì€ ë³µì§€ ì •ì±…ì„ ì´í•´í•˜ëŠ” ë° ì‹¤ì§ˆì ì¸ ê¸¸ì¡ì´ê°€ ë  ê²ƒì…ë‹ˆë‹¤.",
         "ê¶ê¸ˆí–ˆë˜ ë¶€ë¶„ë“¤ì´ í•´ì†Œë˜ë„ë¡ ì•Œì°¨ê²Œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤."]
    ]
    return " ".join(random.choice(p) for p in parts)

def make_last(keyword):
    parts = [
        [f"ì˜¤ëŠ˜ì€ {keyword} ì œë„ë¥¼ {choice('ì•ˆë‚´')}í–ˆìŠµë‹ˆë‹¤.",
         f"ì´ë²ˆ ê¸€ì—ì„œ {keyword}ì˜ í•µì‹¬ ë‚´ìš©ì„ ë‹¤ë¤˜ìŠµë‹ˆë‹¤."],
        [f"ì´ {choice('ì •ë³´')}ë¥¼ ì°¸ê³ í•˜ì…”ì„œ ì‹¤ì œ ì‹ ì²­ì— {choice('ë„ì›€')}ì´ ë˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤.",
         "ê¼­ í•„ìš”í•œ ë¶„ë“¤ì´ í˜œíƒì„ ëˆ„ë¦¬ì‹œê¸¸ ë°”ëë‹ˆë‹¤."],
        [f"ì•ìœ¼ë¡œë„ ë‹¤ì–‘í•œ ë³µì§€ {choice('ì •ë³´')}ë¥¼ ì „í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
         "ìƒí™œ ì†ì—ì„œ ê¼­ í•„ìš”í•œ ì •ë³´ë¥¼ ì „ë‹¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤."],
        ["ëŒ“ê¸€ê³¼ ì˜ê²¬ë„ ë‚¨ê²¨ì£¼ì‹œë©´ í° í˜ì´ ë©ë‹ˆë‹¤.",
         "ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ììœ ë¡­ê²Œ ë‚¨ê²¨ì£¼ì„¸ìš”."],
        ["ì•ìœ¼ë¡œ ë‹¤ë£° ì£¼ì œì— ëŒ€í•œ ì˜ê²¬ë„ ê¸°ë‹¤ë¦¬ê² ìŠµë‹ˆë‹¤.",
         "ê´€ì‹¬ ìˆëŠ” ë‹¤ë¥¸ ë³µì§€ ì œë„ë„ ì°¨ë¡€ì°¨ë¡€ ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤."],
        ["ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ë‹¤ìŒ ê¸€ì—ì„œ ë‹¤ì‹œ ì°¾ì•„ëµ™ê² ìŠµë‹ˆë‹¤.",
         "ëê¹Œì§€ ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦¬ë©°, ë‹¤ìŒ ê¸€ë„ ê¸°ëŒ€í•´ ì£¼ì„¸ìš”."],
        ["ì—¬ëŸ¬ë¶„ì˜ ìƒí™œì´ ë”ìš± ë“ ë“ í•´ì§€ê¸°ë¥¼ ë°”ëë‹ˆë‹¤.",
         "ë³µì§€ ì œë„ë¥¼ í†µí•´ ì‚¶ì´ í•œì¸µ ë‚˜ì•„ì§€ì‹œê¸¸ ê¸°ì›í•©ë‹ˆë‹¤."]
    ]
    return " ".join(random.choice(p) for p in parts)

# ================================
# Blogger ì¸ì¦
# ================================
def get_blogger_service():
    with open("blogger_token.json", "r", encoding="utf-8") as f:
        data = json.load(f)
    creds = UserCredentials.from_authorized_user_info(
        data, ["https://www.googleapis.com/auth/blogger"]
    )
    return build("blogger", "v3", credentials=creds)

blog_handler = get_blogger_service()

# ================================
# ì¶”ì²œê¸€ ë°•ìŠ¤
# ================================
def get_related_posts(blog_id, count=4):
    import feedparser
    rss_url = f"https://www.blogger.com/feeds/{blog_id}/posts/default?alt=rss"
    feed = feedparser.parse(rss_url)
    if not feed.entries: return ""
    entries = random.sample(feed.entries, min(count, len(feed.entries)))
    box = """
<div style="background:#efede9;border-radius:8px;border:2px dashed #a7a297;
            box-shadow:#efede9 0px 0px 0px 10px;color:#565656;font-weight:bold;
            margin:2em 10px;padding:2em;">
  <p data-ke-size="size16"
     style="border-bottom:1px solid #555;color:#555;font-size:16px;
            margin-bottom:15px;padding-bottom:5px;">â™¡â™¥ ê°™ì´ ë³´ë©´ ì¢‹ì€ê¸€</p>
"""
    for entry in entries:
        box += f'<a href="{entry.link}" style="color:#555;font-weight:normal;">â— {entry.title}</a><br>\n'
    return box + "</div>\n"

# ================================
# ë³¸ë¬¸ ìƒì„±
# ================================
data = fetch_welfare_info(wlfareInfoId)
keyword = clean_html(data.get("wlfareInfoNm", "ë³µì§€ ì„œë¹„ìŠ¤"))
title = f"2025 {keyword} ì§€ì› ëŒ€ìƒ ì‹ ì²­ë°©ë²• ì´ì •ë¦¬"

# ì¸ë„¤ì¼ (ì—…ë¡œë“œ ë¡œì§ ë‹¨ìˆœí™” ì˜ˆì‹œ)
safe_keyword = re.sub(r'[\\/:*?"<>|.]', "_", keyword)
folder = f"thumbs/{safe_keyword}"
os.makedirs(folder, exist_ok=True)
png_path = os.path.join(folder, f"{safe_keyword}.png")
make_thumb(png_path, title, 1)
to_webp(png_path)
img_url = f"https://lh3.googleusercontent.com/d/{safe_keyword}"  # ë‹¨ìˆœí™”

intro = make_intro(keyword)
last = make_last(keyword)

html = f"""
<div id="jm">&nbsp;</div>
<p data-ke-size="size18">{intro}</p><br />
<p style="text-align:center;">
    <img src="{img_url}" alt="{keyword} ì¸ë„¤ì¼" style="max-width:100%;height:auto;border-radius:10px;">
</p>
<span><!--more--></span><br />
"""

fields = {
    "ê°œìš”": "wlfareInfoOutlCn",
    "ì§€ì›ëŒ€ìƒ": "wlfareSprtTrgtCn",
    "ì„œë¹„ìŠ¤ë‚´ìš©": "wlfareSprtBnftCn",
    "ì‹ ì²­ë°©ë²•": "aplyMtdDc",
    "ì¶”ê°€ì •ë³´": "etct"
}
for title_k, key in fields.items():
    value = data.get(key, "")
    if not value or value.strip() in ["", "ì •ë³´ ì—†ìŒ"]:
        continue
    processed = process_with_gpt(title_k, clean_html(value), keyword)
    html += f"<br /><h2 data-ke-size='size26'>{keyword} {title_k}</h2><br />{processed}<br /><br />"

related_box = get_related_posts(BLOG_ID)
html += f"""
<div style="margin:40px 0px 20px 0px;">
<p style="text-align:center;" data-ke-size="size18"><a class="myButton" href="{my_url}"> {keyword} </a></p><br />
<p data-ke-size="size18">{last}</p>
</div>
{related_box}
"""

# ================================
# ë¸”ë¡œê·¸ ì—…ë¡œë“œ
# ================================
post_body = {
    "content": html,
    "title": title,
    "labels": ["ë³µì§€", "ì •ë¶€ì§€ì›", "ë³µì§€ì„œë¹„ìŠ¤"],
    "blog": {"id": BLOG_ID},
}
res = blog_handler.posts().insert(blogId=BLOG_ID, body=post_body, isDraft=False, fetchImages=True).execute()
print(f"[ì™„ë£Œ] ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…: {res['url']}")

# ================================
# âœ… êµ¬ê¸€ì‹œíŠ¸ ì—…ë°ì´íŠ¸ (Gì—´/Oì—´)
# ================================
ws.update_cell(target_row, 7, "ì™„")        # Gì—´
ws.update_cell(target_row, 15, res['url']) # Oì—´
print("âœ… êµ¬ê¸€ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ (Gì—´ 'ì™„' + Oì—´ URL ê¸°ë¡)")
print(title)
