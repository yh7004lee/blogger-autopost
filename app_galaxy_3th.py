import requests
from bs4 import BeautifulSoup
import re
import time
import random
import os
import pickle
import json
import urllib.parse
import pyperclip

from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from google.auth.transport.requests import Request
from google.oauth2.service_account import Credentials
from google.oauth2.credentials import Credentials as UserCredentials
import gspread
from openai import OpenAI

# ================================
# OpenAI í´ë¼ì´ì–¸íŠ¸
# ================================
with open("openai.json", "r", encoding="utf-8") as f:
    api_data = json.load(f)
OPENAI_API_KEY = api_data["api_key"]
client = OpenAI(api_key=OPENAI_API_KEY)

# ================================
# Google Sheets ì—°ê²°
# ================================
SERVICE_ACCOUNT_FILE = "sheetapi.json"
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
SHEET_ID = "1SeQogbinIrDTMKjWhGgWPEQq8xv6ARv5n3I-2BsMrSc"  # ì‹¤ì œ ID

def get_sheet():
    creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
    gc = gspread.authorize(creds)
    return gc.open_by_key(SHEET_ID).sheet1

ws = get_sheet()

# ================================
# Google Drive ì¸ì¦
# ================================
def get_drive_service():
    creds = None
    if os.path.exists("drive_token_2nd.pickle"):
        with open("drive_token_2nd.pickle", "rb") as token:
            creds = pickle.load(token)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            raise RuntimeError("drive_token_2nd.pickleì´ ì—†ê±°ë‚˜ ë§Œë£Œë¨. GitHub Secretsì—ì„œ ë³µì› í•„ìš”.")
        with open("drive_token_2nd.pickle", "wb") as token:
            pickle.dump(creds, token)
    return build("drive", "v3", credentials=creds)

# ================================
# Blogger ì¸ì¦
# ================================
def get_blogger_service():
    with open("blogger_token.json", "r", encoding="utf-8") as f:
        data = json.load(f)
    creds = UserCredentials.from_authorized_user_info(data, ["https://www.googleapis.com/auth/blogger"])
    return build("blogger", "v3", credentials=creds)

# ================================
# GPTë¡œ ì•± ì„¤ëª… ë¦¬ë¼ì´íŠ¸
# ================================
def rewrite_app_description(original_html: str, app_name: str, keyword_str: str) -> str:
    compact = BeautifulSoup(original_html, 'html.parser').get_text(separator=' ', strip=True)
    system_msg = (
        "ë„ˆëŠ” í•œêµ­ì–´ ë¸”ë¡œê·¸ ê¸€ì„ ì“°ëŠ” ì¹´í”¼ë¼ì´í„°ì•¼. "
        "ì‚¬ì‹¤ì€ ìœ ì§€í•˜ë˜ ë¬¸ì¥ê³¼ êµ¬ì„±ì„ ì™„ì „íˆ ìƒˆë¡œ ì“°ê³ , "
        "ì‚¬ëŒì´ ì§ì ‘ ì ì€ ë“¯ ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ í†¤ìœ¼ë¡œ í’€ì–´ì¤˜. "
        "ë§ˆí¬ë‹¤ìš´ ê¸ˆì§€, <p data-ke-size=\"size18\"> ë¬¸ë‹¨ë§Œ ì‚¬ìš©. "
        "ì¶œë ¥ì€ ë°˜ë“œì‹œ 3~4ê°œì˜ ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ëˆ ì„œ ì‘ì„±í•˜ë˜, "
        "ê° ë¬¸ë‹¨ ì‚¬ì´ì—ëŠ” <p data-ke-size=\"size18\"> íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ê³  "
        "ë¹ˆ ì¤„(ì¤„ë°”ê¿ˆ)ìœ¼ë¡œ êµ¬ë¶„í•´. "
        "ê° ë¬¸ë‹¨ì€ 3~4ë¬¸ì¥ ì´ë‚´ë¡œë§Œ ì‘ì„±í•´."
    )
    user_msg = (
        f"[ì•±ëª…] {app_name}\n"
        f"[í‚¤ì›Œë“œ] {keyword_str}\n"
        "ì•„ë˜ ì›ë¬¸ì„ ì°¸ê³ í•´ì„œ ë¸”ë¡œê·¸ìš© ì†Œê°œë¬¸ì„ ìƒˆë¡œ ì‘ì„±í•´ì¤˜.\n\n"
        f"{compact}"
    )

    resp = client.chat.completions.create(
        model="gpt-4.1-nano",
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        temperature=0.7,
        max_tokens=450,
    )
    rewritten = resp.choices[0].message.content.strip()
    if "<p" not in rewritten:
        rewritten = f'<p data-ke-size="size18">{rewritten}</p>'
    return rewritten

# ================================
# ì•± ë§í¬ ìˆ˜ì§‘ (requests)
# ================================
def get_app_links(keyword, max_apps=15):
    url = f"https://play.google.com/store/search?q={urllib.parse.quote(keyword)}&c=apps"
    headers = {"User-Agent": "Mozilla/5.0"}
    resp = requests.get(url, headers=headers)
    resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")
    source = soup.find_all("a", class_="Si6A0c Gy4nib")

    app_links = []
    for k, s in enumerate(source):
        if k == max_apps:
            break
        link = "https://play.google.com" + s["href"]
        app_links.append(link)

    # ìƒìœ„ 3ê°œ ì œê±° (ê´‘ê³  ë“±)
    del app_links[:3]
    return app_links

# ================================
# ì•± ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
# ================================
def get_app_detail(app_url, keyword):
    headers = {"User-Agent": "Mozilla/5.0"}
    resp = requests.get(app_url, headers=headers)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    try:
        h1 = soup.find("h1").text.strip()
        h2_title = re.sub(r"[^\uAC00-\uD7A30-9a-zA-Z\s]", "", h1).replace(" ", "")
    except:
        h1 = "ì•±"
        h2_title = h1

    try:
        desc_div = soup.find("div", {"jsname": "sngebd"})
        if desc_div:
            raw_html = str(desc_div)
            contents_text = rewrite_app_description(raw_html, h1, keyword)
        else:
            contents_text = ""
    except Exception as e:
        contents_text = ""
        print(f"[ì–´í”Œì†Œê°œ ì˜¤ë¥˜] {e}")

    images = ""
    try:
        img_tags = soup.select("img.T75of.sHb2Xb")
        for cc, img in enumerate(img_tags[:4], 1):
            img_url = img.get("src")
            if not img_url:
                continue
            img_url = re.sub(r"w\d+-h\d+-rw", "w2048-h1100-rw", img_url)
            images += f'''
            <div class="img-wrap">
              <img src="{img_url}" alt="{h2_title}_{cc}">
            </div>
            '''
    except Exception as e:
        print(f"[ì´ë¯¸ì§€ ì˜¤ë¥˜] {e}")

    return h1, h2_title, contents_text, images

# ================================
# ì‹œíŠ¸ì—ì„œ í‚¤ì›Œë“œ ì½ê¸°
# ================================
row_idx = None
rows = ws.get_all_values()
for idx, row in enumerate(rows[1:], start=2):
    if len(row) < 4 or row[3].strip() != "ì™„":
        row_idx = idx
        break

if not row_idx:
    print("ì²˜ë¦¬í•  í‚¤ì›Œë“œ ì—†ìŒ.")
    exit()

prefix = ws.cell(row_idx, 1).value or ""
keyword = ws.cell(row_idx, 2).value or ""
suffix = ws.cell(row_idx, 3).value or ""
title = f"{prefix} {keyword} {suffix}".strip()

print(f"ğŸ‘‰ ì´ë²ˆ ì‹¤í–‰: {title}")

# ================================
# ì•± í¬ë¡¤ë§ ì‹œì‘
# ================================
app_links = get_app_links(keyword)
print(f"ğŸ‘‰ ìˆ˜ì§‘ëœ ì•± ë§í¬: {len(app_links)}ê°œ")

html = ""
for j, app_url in enumerate(app_links, 1):
    if j > 7:
        break
    h1, h2_title, contents_text, images = get_app_detail(app_url, keyword)

    con_f = f"""
    <h2 data-ke-size="size26">{j}. {h1} ì–´í”Œ ì†Œê°œ</h2>
    <p data-ke-size="size18"><b>1) {h1} ì–´í”Œ ì†Œê°œ</b></p>
    <p data-ke-size="size18">ì´ ì–´í”Œì€ êµ¬ê¸€í”Œë ˆì´ìŠ¤í† ì–´ì—ì„œ "{keyword}" ê²€ìƒ‰ ì‹œ {j}ë²ˆì§¸ë¡œ ë‚˜ì˜¤ëŠ” ì•±ì…ë‹ˆë‹¤.</p>
    """
    con_l = f"""
    <p style="text-align: center;" data-ke-size="size18"><a class="myButton" href="{app_url}"> {h2_title} ì•± ë‹¤ìš´ </a></p>
    <p data-ke-size="size18"><b>2) {h1} ì–´í”Œ ìŠ¤í¬ë¦°ìƒ· </b></p>
    <div class="img-group">{images}</div>
    """
    html += con_f + contents_text + con_l

# ================================
# Blogger ì—…ë¡œë“œ
# ================================
BLOG_IDS = [
    "1271002762142343021",
    "4265887538424434999",
    "6159101125292617147"
]

INDEX_FILE = "last_blog_index.pkl"
if os.path.exists(INDEX_FILE):
    with open(INDEX_FILE, "rb") as f:
        last_index = pickle.load(f)
else:
    last_index = -1

next_index = (last_index + 1) % len(BLOG_IDS)
BLOG_ID = BLOG_IDS[next_index]

with open(INDEX_FILE, "wb") as f:
    pickle.dump(next_index, f)

blogger_service = get_blogger_service()
data_post = {
    "content": html,
    "title": title,
    "labels": ["ì–´í”Œ", "ì•±", "ì¶”ì²œ"],
    "blog": {"id": BLOG_ID},
}
res = blogger_service.posts().insert(blogId=BLOG_ID, body=data_post, isDraft=False, fetchImages=True).execute()
post_url = res.get("url", "")
print("âœ… ì—…ë¡œë“œ ì„±ê³µ:", post_url)

# ================================
# ì‹œíŠ¸ì— ê²°ê³¼ ê¸°ë¡
# ================================
ws.update_cell(row_idx, 4, "ì™„")
ws.update_cell(row_idx, 5, post_url)

# ================================
# ë§ˆë¬´ë¦¬
# ================================
pyperclip.copy(post_url)
print("í´ë¦½ë³´ë“œ ë³µì‚¬ ì™„ë£Œ!")
