import os
import re
import time
import json
import random
import pickle
import traceback
import urllib.parse
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from google.oauth2.service_account import Credentials
from google.oauth2.credentials import Credentials as UserCredentials
from google.auth.transport.requests import Request
import gspread
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.service import Service
from webdriver_manager.firefox import GeckoDriverManager
import sys, io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")
# ================================
# OpenAI ì¸ì¦
# ================================
OPENAI_API_KEY = ""
if os.path.exists("openai.json"):
    with open("openai.json", "r", encoding="utf-8") as f:
        data = json.load(f)
        OPENAI_API_KEY = data.get("api_key", "").strip()
if not OPENAI_API_KEY:
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

# ================================
# Google Sheets ì¸ì¦
# ================================
def get_sheet():
    SERVICE_ACCOUNT_FILE = "sheetapi.json"
    SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
    creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
    gc = gspread.authorize(creds)
    SHEET_ID = os.getenv("SHEET_ID", "1SeQogbinIrDTMKjWhGgWPEQq8xv6ARv5n3I-2BsMrSc")
    return gc.open_by_key(SHEET_ID).sheet1

ws = get_sheet()

# ================================
# ì‹œíŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¡°í•©
# ================================
def get_next_keyword_row():
    rows = ws.get_all_values()
    for i, row in enumerate(rows[1:], start=2):  # 2í–‰ë¶€í„°
        if len(row) < 4:
            continue
        prefix, keyword, suffix, status = row[:4]
        if keyword and (not status or status.strip() != "ì™„"):
            title = f"{prefix.strip()} {keyword.strip()} {suffix.strip()}".strip()
            return i, prefix.strip(), keyword.strip(), suffix.strip(), title
    return None, None, None, None, None

row_idx, prefix, keyword, suffix, title = get_next_keyword_row()
if not keyword:
    print("âœ… ëª¨ë“  í–‰ì´ ì™„ë£Œë¨. ì¢…ë£Œ.")
    exit()

print(f"ğŸ‘‰ ì´ë²ˆ ì‹¤í–‰: {title}")

# ================================
# Google Drive ì¸ì¦
# ================================
def get_drive_service():
    creds = None
    if os.path.exists("drive_token_2nd.pickle"):
        with open("drive_token_2nd.pickle", "rb") as token:
            creds = pickle.load(token)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            raise RuntimeError("âš ï¸ drive_token_2nd.pickleì´ ì—†ê±°ë‚˜ ë§Œë£Œë¨")
        with open("drive_token_2nd.pickle", "wb") as token:
            pickle.dump(creds, token)
    return build("drive", "v3", credentials=creds)

# ================================
# Google Drive ì—…ë¡œë“œ
# ================================
def upload_to_drive(file_path, file_name, folder_name="blogger"):
    drive_service = get_drive_service()
    # blogger í´ë” í™•ì¸
    query = f"mimeType='application/vnd.google-apps.folder' and name='{folder_name}' and trashed=false"
    results = drive_service.files().list(q=query, fields="files(id)").execute()
    items = results.get("files", [])
    if items:
        folder_id = items[0]["id"]
    else:
        folder_metadata = {"name": folder_name, "mimeType": "application/vnd.google-apps.folder"}
        folder = drive_service.files().create(body=folder_metadata, fields="id").execute()
        folder_id = folder["id"]

    media = MediaFileUpload(file_path, mimetype="image/webp", resumable=True)
    file = drive_service.files().create(body={"name": file_name, "parents": [folder_id]},
                                        media_body=media, fields="id").execute()
    drive_service.permissions().create(fileId=file["id"],
                                       body={"role": "reader", "type": "anyone"}).execute()
    return f"https://lh3.googleusercontent.com/d/{file['id']}"

# ================================
# Blogger ì¸ì¦
# ================================
def get_blogger_service():
    with open("blogger_token.json", "r", encoding="utf-8") as f:
        data = json.load(f)
    creds = UserCredentials.from_authorized_user_info(data, ["https://www.googleapis.com/auth/blogger"])
    return build("blogger", "v3", credentials=creds)

# ================================
# GPTë¡œ ì•± ì†Œê°œë¬¸ ìƒˆë¡œì“°ê¸°
# ================================
def rewrite_app_description(original_html: str, app_name: str, keyword_str: str) -> str:
    compact = BeautifulSoup(original_html, "html.parser").get_text(separator=" ", strip=True)
    system_msg = (
        "ë„ˆëŠ” í•œêµ­ì–´ ë¸”ë¡œê·¸ ê¸€ì„ ì“°ëŠ” ì¹´í”¼ë¼ì´í„°ì•¼. "
        "ì‚¬ì‹¤ì€ ìœ ì§€í•˜ë˜ ë¬¸ì¥ê³¼ êµ¬ì„±ì„ ì™„ì „íˆ ìƒˆë¡œ ì“°ê³ , "
        "ì‚¬ëŒì´ ì§ì ‘ ì ì€ ë“¯ ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ í†¤ìœ¼ë¡œ í’€ì–´ì¤˜. "
        "ë§ˆí¬ë‹¤ìš´ ê¸ˆì§€, <p data-ke-size=\"size18\"> ë¬¸ë‹¨ë§Œ ì‚¬ìš©. "
        "ì¶œë ¥ì€ ë°˜ë“œì‹œ 3~4ê°œì˜ ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ëˆ ì„œ ì‘ì„±í•˜ë˜, "
        "ê° ë¬¸ë‹¨ì€ 3~4ë¬¸ì¥ ì´ë‚´ë¡œë§Œ ì‘ì„±í•´."
    )
    user_msg = f"[ì•±ëª…] {app_name}\n[í‚¤ì›Œë“œ] {keyword_str}\nì•„ë˜ ì›ë¬¸ì„ ì°¸ê³ í•´ì„œ ë¸”ë¡œê·¸ìš© ì†Œê°œë¬¸ì„ ìƒˆë¡œ ì‘ì„±í•´ì¤˜.\n\n{compact}"

    resp = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        temperature=0.7,
        max_tokens=500,
    )
    rewritten = resp.choices[0].message.content.strip()
    if "<p" not in rewritten:
        rewritten = f'<p data-ke-size="size18">{rewritten}</p>'
    return rewritten

# ================================
# Selenium ì‹¤í–‰ (Firefox headless)
# ================================
options = webdriver.FirefoxOptions()
options.add_argument("--headless")
service = Service(executable_path=GeckoDriverManager().install())
chrome = webdriver.Firefox(service=service, options=options)
chrome.implicitly_wait(10)

# ================================
# ì•± í¬ë¡¤ë§ ì‹œì‘
# ================================
url = f"https://play.google.com/store/search?q={keyword}&c=apps"
chrome.get(url)
time.sleep(2)
soup = BeautifulSoup(chrome.page_source, "html.parser")
source = soup.find_all(class_="ULeU3b")

app_links = []
for s in source[:15]:
    link = "https://play.google.com" + s.find("a")["href"]
    app_links.append(link)

del app_links[:3]  # ê´‘ê³  ì œê±°
print(f"ğŸ‘‰ ìˆ˜ì§‘ëœ ì•± ë§í¬: {len(app_links)}ê°œ")

# ================================
# ì„œë¡ 
# ================================
intro = f"""
<div id="jm">&nbsp;</div>
<p data-ke-size="size18">
ìŠ¤ë§ˆíŠ¸í°ë§Œ ìˆìœ¼ë©´ ìƒí™œì´ í›¨ì”¬ í¸ë¦¬í•´ì§‘ë‹ˆë‹¤. ì´ë²ˆ ê¸€ì—ì„œëŠ” "{title}" ê´€ë ¨ ì•±ë“¤ì„ ì§‘ì¤‘ì ìœ¼ë¡œ ì†Œê°œí•©ë‹ˆë‹¤. 
êµ¬ê¸€í”Œë ˆì´ìŠ¤í† ì–´ì—ì„œ "{keyword}" ê²€ìƒ‰ ì‹œ ìƒìœ„ ë…¸ì¶œë˜ëŠ” ì•±ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ì„ ì •í–ˆìœ¼ë©°, 
ê° ì•±ì˜ íŠ¹ì§•ê³¼ ì‹¤ì œ ì‚¬ìš© í›„ê¸°ë¥¼ í•¨ê»˜ ë‹´ì•„ ëê¹Œì§€ ì½ìœ¼ì‹œë©´ ì•± ì„ íƒì— í° ë„ì›€ì´ ë  ê²ƒì…ë‹ˆë‹¤.
</p>
<span><!--more--></span>
"""

html = intro

# ================================
# ì•±ë³„ HTML ìƒì„±
# ================================
for j, app_url in enumerate(app_links, 1):
    if j > 5:
        break
    chrome.get(app_url)
    time.sleep(1)
    soup = BeautifulSoup(chrome.page_source, "html.parser")
    try:
        h1 = soup.find("h1").text.strip()
    except:
        h1 = f"ì•±{j}"
    try:
        desc_div = soup.find("div", class_="bARER") or soup.find("div", class_="fysCi")
        raw_html = str(desc_div) if desc_div else ""
        contents_text = rewrite_app_description(raw_html, h1, keyword)
    except Exception:
        contents_text = ""

    html += f"""
    <h2 data-ke-size="size26">{j}. {h1} ì–´í”Œ ì†Œê°œ</h2>
    {contents_text}
    <p style="text-align:center;" data-ke-size="size18"><a class="myButton" href="{app_url}">ğŸ‘‰ {h1} ë‹¤ìš´ë¡œë“œ</a></p>
    """

# ================================
# ë§ˆë¬´ë¦¬
# ================================
html += f"""
<div style="margin:40px 0 20px 0;">
<p data-ke-size="size18">
ì˜¤ëŠ˜ ì†Œê°œí•œ "{title}" ê´€ë ¨ ì•±ë“¤ì´ ì¼ìƒ ì†ì—ì„œ ìœ ìš©í•˜ê²Œ í™œìš©ë˜ê¸¸ ë°”ëë‹ˆë‹¤. 
ì•ìœ¼ë¡œë„ ë‹¤ì–‘í•œ ì•± ì •ë³´ë¥¼ ê¾¸ì¤€íˆ ì „í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. 
ëŒ“ê¸€ê³¼ ì˜ê²¬ë„ ë‚¨ê²¨ì£¼ì‹œë©´ í° í˜ì´ ë©ë‹ˆë‹¤.
</p>
</div>
"""

# ================================
# Blogger ì—…ë¡œë“œ
# ================================
BLOG_IDS = ["1271002762142343021", "4265887538424434999", "6159101125292617147"]
index_file = "last_blog_index.pkl"
last_index = -1
if os.path.exists(index_file):
    with open(index_file, "rb") as f:
        last_index = pickle.load(f)
next_index = (last_index + 1) % len(BLOG_IDS)
BLOG_ID = BLOG_IDS[next_index]

with open(index_file, "wb") as f:
    pickle.dump(next_index, f)

blogger = get_blogger_service()
post_body = {"content": html, "title": title, "labels": ["ì•±", "ì–´í”Œ", "ì¶”ì²œ"], "blog": {"id": BLOG_ID}}
res = blogger.posts().insert(blogId=BLOG_ID, body=post_body, isDraft=False, fetchImages=True).execute()

print("âœ… ì—…ë¡œë“œ ì„±ê³µ:", res["url"])

# âœ… ì—‘ì…€/ì‹œíŠ¸ì— ê²°ê³¼ ê¸°ë¡
ws.update_cell(row_idx, 4, "ì™„")       # Dì—´ "ì™„"
ws.update_cell(row_idx, 5, post_url)   # Eì—´ì— í¬ìŠ¤íŒ… URL ê¸°ë¡

chrome.quit()



